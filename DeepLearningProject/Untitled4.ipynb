{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Reviews prediction**"
      ],
      "metadata": {
        "id": "MpoQA6mjZyka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install scikeras\n",
        "!pip install keras-tuner"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Bs-RN5IDtD",
        "outputId": "653eb9d8-17a1-453b-9aff-27c06425c961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.10/dist-packages (0.12.0)\n",
            "Requirement already satisfied: packaging>=0.21 in /usr/local/lib/python3.10/dist-packages (from scikeras) (23.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikeras) (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikeras) (3.2.0)\n",
            "Requirement already satisfied: keras-tuner in /usr/local/lib/python3.10/dist-packages (1.4.6)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.15.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (23.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (2.31.0)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.10/dist-packages (from keras-tuner) (1.0.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->keras-tuner) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#imports\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "import gensim\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Dense, Embedding, Dropout, Input\n",
        "from sklearn.metrics import f1_score\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from kerastuner.tuners import RandomSearch\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras_tuner import Objective\n",
        "from kerastuner.tuners import RandomSearch\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import make_scorer"
      ],
      "metadata": {
        "id": "Xg-4y7W5XwTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uploading the dataset"
      ],
      "metadata": {
        "id": "ODMGJTr5Wcc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTIrMba7JTUe"
      },
      "outputs": [],
      "source": [
        "# ZIP data URL from GitHub\n",
        "zip_url = \"https://github.com/sandyj02/projects/raw/main/DeepLearningProject/input.zip\"\n",
        "\n",
        "# Download the ZIP\n",
        "zip_path, _ = urllib.request.urlretrieve(zip_url)\n",
        "\n",
        "# Unzip the folder\n",
        "target_folder = \"temp\"\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(target_folder)\n",
        "\n",
        "data_path = f\"{target_folder}/input.pkl\"\n",
        "\n",
        "import pickle as pk\n",
        "\n",
        "# Open the pickle data\n",
        "with open(data_path, 'rb') as f:\n",
        "  train_labels, test_labels, train_texts, test_texts, train_bow, test_bow = pk.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm going to start by taking a look at some of the data that we are dealing with.\n",
        "\n",
        "2) I'm taking into account only text as a input since I deemed the bag of words field to be inefficient because of its sparsity.\n",
        "\n"
      ],
      "metadata": {
        "id": "qz5F7cM_aCcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#taking a look at the length of the train and test sets\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5MbVBMGD-WL",
        "outputId": "8df74ff2-3afe-47c9-f694-7f6c677a5a4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(560, 560, 140, 140)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taking a look at how many instances belong to positive or negative class\n",
        "count_pos, count_neg= 0, 0\n",
        "\n",
        "for i in train_labels:\n",
        "  if i=='positive':\n",
        "    count_pos+=1\n",
        "  else:\n",
        "    count_neg+=1\n",
        "\n",
        "print(f\"class 1: {count_pos} \\nclass 0: {count_neg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJoFk3p6Eh5L",
        "outputId": "8b3f3ef2-e66a-41cd-9cb6-b924674710c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 1: 398 \n",
            "class 0: 162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#taking at look at samples of data\n",
        "for i in range (4):\n",
        "  print(train_labels[i] + \" | \" + train_texts[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqWyoVCqY7mb",
        "outputId": "89f0669b-a41a-4c22-d7b4-141d88b4eb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive | Nifty little episode played mainly for laughs, but with clever dollop of suspense. Somehow a Martian has snuck aboard a broken-down bus on its way to nowhere, but which passenger is it, (talk about your illegal immigrants!). All-star supporting cast, from wild-eyed Jack Elam (hamming it up shamelessly), to sexy Jean Willes (if she's the Martian, then I say let's open the borders!), to cruel-faced John Hoyt (the most obvious suspect), along with familiar faces John Archer and Barney Phillips (and a nice turn from Bill Kendis as the bus driver). Makes for a very entertaining half-hour even if the action is confined to a single set.\n",
            "positive | this is one of the finest movies i have ever seen....the stark scenery...the isolation...the ignorant bigoted people hiding behind their religion...a backdrop for some wordliness and sophistication...the acting is completely natural...but for me as a\"foodie' the best is the actual choosing and preparation of the feast..i have spent time in paris and know the cuisine well...whether or not the cafe anglais really exists i don't know but i do know of similar establishments and babette's menu and choice of wines are authentic...and of course the end where despite themselves the perfect meal mellows them back to friendship is the only ending there could be..this is a 10 out of 10 film and should be seen by anyone with enough brain and taste to understand it\n",
            "positive | I've read the other reviews and found some to be comparison of movie v real life (eg what it takes to get into music school), Britney Bashing, etc, etc. so let's focus on the movie and the message.<br /><br />I have rated this movie 7 out of 10 for the age range 8 to 14 years, and for a family movie. For the average adult male.... 2 out of 10.<br /><br />I like pop/rock music, i'm 45. I know of Britney Spears but never realised she actually sang Stronger until i read the credits and these reviews. I didn't recognise her poster on the wall so I was not worried about any 'self promotion'.<br /><br />I watch movies to be entertained. i don't care about casting, lighting, producers, directors, etc. What is the movie and does it entertain me.<br /><br />I watched this movie for the message. The world's greatest epidemic is low self-esteem (which is a whole other story) so watched with the message in mind, as that is an area of interest. The movie is light, bright and breezy, great for kids. I found the Texan twang began to fade throughout the movie and of course there are only so many ways to convey the give up/don't give up message, so yeh, it was a bit predictable. Great message though...should be more of them.<br /><br />This movie is a great family movie, but for a bloke watching by himself, get Hannibal.\n",
            "negative | I was looking forward to this so much, being a big fan of the book. However, when it came out I remember thinking it was one of the biggest wastes of money and time I've ever spent at the cinema.<br /><br />In principle, the acting, the sets and the music were excellent, and are the main reason why I'm rating this a 4.<br /><br />In this version, Sara is a little too self-sacrificing for my taste. There is no way she would have deliberately lied to Miss Minchin just to stop her punishing the other girls; in the book she makes a point of describing lies as \"not just wicked, but vulgar.\" <br /><br />There's also far too much of a Disneyfied ending for me; Sara's father coming back from the dead and all of them trotting off into the Indian sunset. While the book does have a happy (and critics might say equally improbable) ending, it doesn't leave you thinking, \"Oh puh-leeze.\"<br /><br />About the only things true to the book were:<br /><br />1. Sara's father being a soldier 2. The lines between Sara and her father (\"Are you learning me by heart?\"/\"No. I know you by heart. You are inside my heart.\") 3. Sara's friendship with Becky, and her 'adopting' Lottie (although this last one wasn't developed as much as it could have been) 4. The changing of her room by adding various luxury items. That part was brilliantly done. 5. The basic core - a rich girl being flung into poverty suddenly - is there, but that's about all that is.<br /><br />People might say that this adaptation is more for the younger audience. Possibly. All I can say to that is I have two cousins - aged 7 and 12 respectively - who were big fans of this film until they read the book.<br /><br />If all you want is a 'feel-good' family film, then this delivers. If you're looking for a film that actually tells the story of A Little Princess (in fact, if you've read the book) don't waste time with this one. It's such a shame; with a cast like this, if they'd stuck to at least the basic story it could have been fantastic.<br /><br />Am I harping on about 'read the book' this and 'read the book' that a little too much? Very probably. But if someone attempts to adapt a book - especially such a classic - into a movie, then they should at least have done the same thing. Preferably more than once.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "This was not mentioned in my exam but since i noticed some html tags i'm going to delete them since the model may not know how to preprocess them"
      ],
      "metadata": {
        "id": "CXys_pyVfJsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "def html_tags_removal(text_list):\n",
        "  stripped_text=[]\n",
        "  for i in text_list:\n",
        "    soup = BeautifulSoup(i, \"html.parser\")\n",
        "    stripping = soup.get_text(separator=\" \")\n",
        "    stripped_text.append(stripping)\n",
        "  return stripped_text\n",
        "\n",
        "train_stripped_text=html_tags_removal(train_texts)\n",
        "test_stripped_text=html_tags_removal(test_texts)\n",
        "\n",
        "#examples text of before and after tags removal\n",
        "print(f\"original: {train_texts[2]} \\ncorrected: {train_stripped_text[2]}\\n\")\n",
        "print(f\"original: {test_texts[0]} \\ncorrected: {test_stripped_text[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LNV6B1ZhJEi",
        "outputId": "b95edd7e-14be-49c4-a9e5-9a0152c3aeac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original: I've read the other reviews and found some to be comparison of movie v real life (eg what it takes to get into music school), Britney Bashing, etc, etc. so let's focus on the movie and the message.<br /><br />I have rated this movie 7 out of 10 for the age range 8 to 14 years, and for a family movie. For the average adult male.... 2 out of 10.<br /><br />I like pop/rock music, i'm 45. I know of Britney Spears but never realised she actually sang Stronger until i read the credits and these reviews. I didn't recognise her poster on the wall so I was not worried about any 'self promotion'.<br /><br />I watch movies to be entertained. i don't care about casting, lighting, producers, directors, etc. What is the movie and does it entertain me.<br /><br />I watched this movie for the message. The world's greatest epidemic is low self-esteem (which is a whole other story) so watched with the message in mind, as that is an area of interest. The movie is light, bright and breezy, great for kids. I found the Texan twang began to fade throughout the movie and of course there are only so many ways to convey the give up/don't give up message, so yeh, it was a bit predictable. Great message though...should be more of them.<br /><br />This movie is a great family movie, but for a bloke watching by himself, get Hannibal. \n",
            "corrected: I've read the other reviews and found some to be comparison of movie v real life (eg what it takes to get into music school), Britney Bashing, etc, etc. so let's focus on the movie and the message. I have rated this movie 7 out of 10 for the age range 8 to 14 years, and for a family movie. For the average adult male.... 2 out of 10. I like pop/rock music, i'm 45. I know of Britney Spears but never realised she actually sang Stronger until i read the credits and these reviews. I didn't recognise her poster on the wall so I was not worried about any 'self promotion'. I watch movies to be entertained. i don't care about casting, lighting, producers, directors, etc. What is the movie and does it entertain me. I watched this movie for the message. The world's greatest epidemic is low self-esteem (which is a whole other story) so watched with the message in mind, as that is an area of interest. The movie is light, bright and breezy, great for kids. I found the Texan twang began to fade throughout the movie and of course there are only so many ways to convey the give up/don't give up message, so yeh, it was a bit predictable. Great message though...should be more of them. This movie is a great family movie, but for a bloke watching by himself, get Hannibal.\n",
            "\n",
            "original: Thought provoking, humbling depiction of the human tragedies of war. A small, but altruistic view of one family's interactions with the enemy during the civil war in Kentucky. This movie lessens the \"glamor\" of war; showing it's effect on not only the soldier but the entire family unit.<br /><br />A lot of today's movies show war as an opportunity to highlight the \"hero's\" and other glamorous features of war, but very little attempts to show the true effect war actually takes on a community. This movie attempts this through a retelling of a person's memory of those days. This movie is stated to be loose translation of an actual events, when in reality, this movie is probably a factual reality of hundreds, perhaps thousands of \"actual events\" during the civil war. I highly recommend those interested in our civil war to watch this movie. \n",
            "corrected: Thought provoking, humbling depiction of the human tragedies of war. A small, but altruistic view of one family's interactions with the enemy during the civil war in Kentucky. This movie lessens the \"glamor\" of war; showing it's effect on not only the soldier but the entire family unit. A lot of today's movies show war as an opportunity to highlight the \"hero's\" and other glamorous features of war, but very little attempts to show the true effect war actually takes on a community. This movie attempts this through a retelling of a person's memory of those days. This movie is stated to be loose translation of an actual events, when in reality, this movie is probably a factual reality of hundreds, perhaps thousands of \"actual events\" during the civil war. I highly recommend those interested in our civil war to watch this movie.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Now I perform tokenization and punctuation removal of the text with simple regex"
      ],
      "metadata": {
        "id": "MGNJtOHx2V9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_remove_punctuation(text_list):\n",
        "  tokenized_text = []\n",
        "  for i in text_list:\n",
        "      tokens = re.split('\\W+', i)\n",
        "      tokenized_text.append(tokens)\n",
        "  return tokenized_text\n",
        "\n",
        "train_tokenized_text = tokenize_remove_punctuation(train_stripped_text)\n",
        "test_tokenized_text = tokenize_remove_punctuation(test_stripped_text)\n",
        "\n",
        "#examples text after tokenization and punctuation removal\n",
        "print(train_tokenized_text[2])\n",
        "print(test_tokenized_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PnczXuEZMS5",
        "outputId": "78ce072a-33c9-4f70-c724-bd5bad4b0a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 've', 'read', 'the', 'other', 'reviews', 'and', 'found', 'some', 'to', 'be', 'comparison', 'of', 'movie', 'v', 'real', 'life', 'eg', 'what', 'it', 'takes', 'to', 'get', 'into', 'music', 'school', 'Britney', 'Bashing', 'etc', 'etc', 'so', 'let', 's', 'focus', 'on', 'the', 'movie', 'and', 'the', 'message', 'I', 'have', 'rated', 'this', 'movie', '7', 'out', 'of', '10', 'for', 'the', 'age', 'range', '8', 'to', '14', 'years', 'and', 'for', 'a', 'family', 'movie', 'For', 'the', 'average', 'adult', 'male', '2', 'out', 'of', '10', 'I', 'like', 'pop', 'rock', 'music', 'i', 'm', '45', 'I', 'know', 'of', 'Britney', 'Spears', 'but', 'never', 'realised', 'she', 'actually', 'sang', 'Stronger', 'until', 'i', 'read', 'the', 'credits', 'and', 'these', 'reviews', 'I', 'didn', 't', 'recognise', 'her', 'poster', 'on', 'the', 'wall', 'so', 'I', 'was', 'not', 'worried', 'about', 'any', 'self', 'promotion', 'I', 'watch', 'movies', 'to', 'be', 'entertained', 'i', 'don', 't', 'care', 'about', 'casting', 'lighting', 'producers', 'directors', 'etc', 'What', 'is', 'the', 'movie', 'and', 'does', 'it', 'entertain', 'me', 'I', 'watched', 'this', 'movie', 'for', 'the', 'message', 'The', 'world', 's', 'greatest', 'epidemic', 'is', 'low', 'self', 'esteem', 'which', 'is', 'a', 'whole', 'other', 'story', 'so', 'watched', 'with', 'the', 'message', 'in', 'mind', 'as', 'that', 'is', 'an', 'area', 'of', 'interest', 'The', 'movie', 'is', 'light', 'bright', 'and', 'breezy', 'great', 'for', 'kids', 'I', 'found', 'the', 'Texan', 'twang', 'began', 'to', 'fade', 'throughout', 'the', 'movie', 'and', 'of', 'course', 'there', 'are', 'only', 'so', 'many', 'ways', 'to', 'convey', 'the', 'give', 'up', 'don', 't', 'give', 'up', 'message', 'so', 'yeh', 'it', 'was', 'a', 'bit', 'predictable', 'Great', 'message', 'though', 'should', 'be', 'more', 'of', 'them', 'This', 'movie', 'is', 'a', 'great', 'family', 'movie', 'but', 'for', 'a', 'bloke', 'watching', 'by', 'himself', 'get', 'Hannibal', '']\n",
            "['Thought', 'provoking', 'humbling', 'depiction', 'of', 'the', 'human', 'tragedies', 'of', 'war', 'A', 'small', 'but', 'altruistic', 'view', 'of', 'one', 'family', 's', 'interactions', 'with', 'the', 'enemy', 'during', 'the', 'civil', 'war', 'in', 'Kentucky', 'This', 'movie', 'lessens', 'the', 'glamor', 'of', 'war', 'showing', 'it', 's', 'effect', 'on', 'not', 'only', 'the', 'soldier', 'but', 'the', 'entire', 'family', 'unit', 'A', 'lot', 'of', 'today', 's', 'movies', 'show', 'war', 'as', 'an', 'opportunity', 'to', 'highlight', 'the', 'hero', 's', 'and', 'other', 'glamorous', 'features', 'of', 'war', 'but', 'very', 'little', 'attempts', 'to', 'show', 'the', 'true', 'effect', 'war', 'actually', 'takes', 'on', 'a', 'community', 'This', 'movie', 'attempts', 'this', 'through', 'a', 'retelling', 'of', 'a', 'person', 's', 'memory', 'of', 'those', 'days', 'This', 'movie', 'is', 'stated', 'to', 'be', 'loose', 'translation', 'of', 'an', 'actual', 'events', 'when', 'in', 'reality', 'this', 'movie', 'is', 'probably', 'a', 'factual', 'reality', 'of', 'hundreds', 'perhaps', 'thousands', 'of', 'actual', 'events', 'during', 'the', 'civil', 'war', 'I', 'highly', 'recommend', 'those', 'interested', 'in', 'our', 'civil', 'war', 'to', 'watch', 'this', 'movie', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Now we perform case folding"
      ],
      "metadata": {
        "id": "mPQtIl0r8JXe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def case_folding(text):\n",
        "  case_folding_list = []\n",
        "  for sentence in text:\n",
        "      folded_sentence = [word.lower() for word in sentence]\n",
        "      case_folding_list.append(folded_sentence)\n",
        "  return case_folding_list\n",
        "\n",
        "train_lower=case_folding(train_tokenized_text)\n",
        "test_lower=case_folding(test_tokenized_text)\n",
        "\n",
        "#examples of case foleded text\n",
        "print(train_lower[2])\n",
        "print(test_lower[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqBEtrz2400S",
        "outputId": "b0d66931-5503-4c11-b65b-9225296caac4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 've', 'read', 'the', 'other', 'reviews', 'and', 'found', 'some', 'to', 'be', 'comparison', 'of', 'movie', 'v', 'real', 'life', 'eg', 'what', 'it', 'takes', 'to', 'get', 'into', 'music', 'school', 'britney', 'bashing', 'etc', 'etc', 'so', 'let', 's', 'focus', 'on', 'the', 'movie', 'and', 'the', 'message', 'i', 'have', 'rated', 'this', 'movie', '7', 'out', 'of', '10', 'for', 'the', 'age', 'range', '8', 'to', '14', 'years', 'and', 'for', 'a', 'family', 'movie', 'for', 'the', 'average', 'adult', 'male', '2', 'out', 'of', '10', 'i', 'like', 'pop', 'rock', 'music', 'i', 'm', '45', 'i', 'know', 'of', 'britney', 'spears', 'but', 'never', 'realised', 'she', 'actually', 'sang', 'stronger', 'until', 'i', 'read', 'the', 'credits', 'and', 'these', 'reviews', 'i', 'didn', 't', 'recognise', 'her', 'poster', 'on', 'the', 'wall', 'so', 'i', 'was', 'not', 'worried', 'about', 'any', 'self', 'promotion', 'i', 'watch', 'movies', 'to', 'be', 'entertained', 'i', 'don', 't', 'care', 'about', 'casting', 'lighting', 'producers', 'directors', 'etc', 'what', 'is', 'the', 'movie', 'and', 'does', 'it', 'entertain', 'me', 'i', 'watched', 'this', 'movie', 'for', 'the', 'message', 'the', 'world', 's', 'greatest', 'epidemic', 'is', 'low', 'self', 'esteem', 'which', 'is', 'a', 'whole', 'other', 'story', 'so', 'watched', 'with', 'the', 'message', 'in', 'mind', 'as', 'that', 'is', 'an', 'area', 'of', 'interest', 'the', 'movie', 'is', 'light', 'bright', 'and', 'breezy', 'great', 'for', 'kids', 'i', 'found', 'the', 'texan', 'twang', 'began', 'to', 'fade', 'throughout', 'the', 'movie', 'and', 'of', 'course', 'there', 'are', 'only', 'so', 'many', 'ways', 'to', 'convey', 'the', 'give', 'up', 'don', 't', 'give', 'up', 'message', 'so', 'yeh', 'it', 'was', 'a', 'bit', 'predictable', 'great', 'message', 'though', 'should', 'be', 'more', 'of', 'them', 'this', 'movie', 'is', 'a', 'great', 'family', 'movie', 'but', 'for', 'a', 'bloke', 'watching', 'by', 'himself', 'get', 'hannibal', '']\n",
            "['thought', 'provoking', 'humbling', 'depiction', 'of', 'the', 'human', 'tragedies', 'of', 'war', 'a', 'small', 'but', 'altruistic', 'view', 'of', 'one', 'family', 's', 'interactions', 'with', 'the', 'enemy', 'during', 'the', 'civil', 'war', 'in', 'kentucky', 'this', 'movie', 'lessens', 'the', 'glamor', 'of', 'war', 'showing', 'it', 's', 'effect', 'on', 'not', 'only', 'the', 'soldier', 'but', 'the', 'entire', 'family', 'unit', 'a', 'lot', 'of', 'today', 's', 'movies', 'show', 'war', 'as', 'an', 'opportunity', 'to', 'highlight', 'the', 'hero', 's', 'and', 'other', 'glamorous', 'features', 'of', 'war', 'but', 'very', 'little', 'attempts', 'to', 'show', 'the', 'true', 'effect', 'war', 'actually', 'takes', 'on', 'a', 'community', 'this', 'movie', 'attempts', 'this', 'through', 'a', 'retelling', 'of', 'a', 'person', 's', 'memory', 'of', 'those', 'days', 'this', 'movie', 'is', 'stated', 'to', 'be', 'loose', 'translation', 'of', 'an', 'actual', 'events', 'when', 'in', 'reality', 'this', 'movie', 'is', 'probably', 'a', 'factual', 'reality', 'of', 'hundreds', 'perhaps', 'thousands', 'of', 'actual', 'events', 'during', 'the', 'civil', 'war', 'i', 'highly', 'recommend', 'those', 'interested', 'in', 'our', 'civil', 'war', 'to', 'watch', 'this', 'movie', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) And finally as the last step of preprocessing let's perform word embeddings with word2vec. By doing so we obtain a numerical representation of the text to feed to the model"
      ],
      "metadata": {
        "id": "6Ui44k3VGIcl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = gensim.models.Word2Vec(train_lower, vector_size=200, window=10, min_count=2)\n",
        "\n",
        "def get_sentence_embedding(sentence_tokens, model):\n",
        "  word_embeddings = [model.wv[word] for word in sentence_tokens if word in model.wv]\n",
        "  if not word_embeddings:\n",
        "      return np.zeros(model.vector_size)\n",
        "  else:\n",
        "      return np.mean(word_embeddings, axis=0)\n",
        "\n",
        "train_sentence_embeddings = [get_sentence_embedding(sentence, model) for sentence in train_lower]\n",
        "test_sentence_embeddings = [get_sentence_embedding(sentence, model) for sentence in test_lower]\n",
        "\n",
        "X_train = np.array(train_sentence_embeddings)\n",
        "X_test = np.array(test_sentence_embeddings)\n",
        "\n",
        "X_train, X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FGvx3m5B5Sp",
        "outputId": "fa1fe485-4424-43d6-ec91-e2e487c70ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[ 0.06798946, -0.01916741, -0.03105113, ..., -0.4840906 ,\n",
              "         -0.07015246, -0.24227408],\n",
              "        [ 0.08075116, -0.02504006, -0.03707635, ..., -0.5736068 ,\n",
              "         -0.08399564, -0.29066166],\n",
              "        [ 0.07056703, -0.01964231, -0.04766725, ..., -0.55007607,\n",
              "         -0.07300784, -0.2992239 ],\n",
              "        ...,\n",
              "        [ 0.07738679, -0.02201782, -0.04605547, ..., -0.5793911 ,\n",
              "         -0.07993756, -0.30869502],\n",
              "        [ 0.07370503, -0.02138978, -0.04088891, ..., -0.54972833,\n",
              "         -0.07675394, -0.28723934],\n",
              "        [ 0.07827501, -0.02214917, -0.04601169, ..., -0.58095205,\n",
              "         -0.0788158 , -0.30790952]], dtype=float32),\n",
              " array([[ 0.08025537, -0.02635876, -0.0316317 , ..., -0.54715   ,\n",
              "         -0.08206019, -0.2724247 ],\n",
              "        [ 0.08257303, -0.02382403, -0.03927889, ..., -0.59153897,\n",
              "         -0.08506055, -0.3003761 ],\n",
              "        [ 0.06673774, -0.01721528, -0.0496358 , ..., -0.53053546,\n",
              "         -0.06690422, -0.29327458],\n",
              "        ...,\n",
              "        [ 0.07260589, -0.01967866, -0.04867552, ..., -0.55061996,\n",
              "         -0.07119451, -0.29990038],\n",
              "        [ 0.07617817, -0.02267985, -0.03663436, ..., -0.547561  ,\n",
              "         -0.07899193, -0.28074783],\n",
              "        [ 0.08360468, -0.02706674, -0.03739261, ..., -0.5898111 ,\n",
              "         -0.08666309, -0.2946135 ]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Now let's convert the labels lists into binary values (positive=1, negative=0)"
      ],
      "metadata": {
        "id": "oGBfvYbKfM1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_labels_to_binary(labels):\n",
        "  binary_labels = [1 if label == 'positive' else 0 for label in labels]\n",
        "  return binary_labels\n",
        "\n",
        "binary_train=convert_labels_to_binary(train_labels)\n",
        "binary_test=convert_labels_to_binary(test_labels)\n",
        "\n",
        "Y_train=np.array(binary_train)\n",
        "Y_test=np.array(binary_test)\n",
        "\n",
        "print (train_labels[:5])\n",
        "print (Y_train[:5])\n",
        "print (test_labels[:5])\n",
        "print (Y_test[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7I4h_VNKVF25",
        "outputId": "d7952d88-daf2-4059-8e6f-0e4a2e9a488f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['positive', 'positive', 'positive', 'negative', 'negative']\n",
            "[1 1 1 0 0]\n",
            "['positive', 'negative', 'positive', 'positive', 'positive']\n",
            "[1 0 1 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "lp6ke-dsAEOx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) The model that I'm going to use to make prediction on the reviews is bidirectional reccurent neural network"
      ],
      "metadata": {
        "id": "5F6d7nZicxr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshaping\n",
        "num_train_samples, embedding_dim = X_train.shape\n",
        "X_train_reshaped = X_train.reshape((num_train_samples, 1,  embedding_dim))\n",
        "num_test_samples, embedding_dim = X_test.shape\n",
        "X_test_reshaped = X_test.reshape((num_test_samples, 1,  embedding_dim))"
      ],
      "metadata": {
        "id": "qh3VBSLLIgeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)\n",
        " - The model will be composed of bidirectional rnn layers and dense layers\n",
        " - I will use ReLu as an activation function for the hidden layers\n",
        " - I will use drouput for regularization and i will be using HE initializer since it works well with  ReLU"
      ],
      "metadata": {
        "id": "mx3fI9htlOpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 10\n",
        "tf.random.set_seed(random_seed)\n",
        "\n",
        "def f1_metric(y_true, y_pred):\n",
        "  y_pred_binary = tf.cast(y_pred > 0.5, tf.int32)\n",
        "  return tf.py_function(f1_score, (y_true, y_pred_binary), tf.float64)\n",
        "\n",
        "model = Sequential([\n",
        "  Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), input_shape=( X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
        "  Dropout(0.5),\n",
        "  Bidirectional(LSTM(256, kernel_initializer='he_normal', return_sequences=True)),\n",
        "  Dropout(0.5),\n",
        "  Bidirectional(LSTM(128, kernel_initializer='he_normal')),\n",
        "  Dropout(0.5),\n",
        "  Dense(64, activation='relu', kernel_initializer='he_normal'), #I'm using re_lu as an activation function for hidden layers to introduce non linearity\n",
        "  Dropout(0.5),\n",
        "  Dense(1, activation='sigmoid')  #3)The output layer has sigmoid as an activation function\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[f1_metric])\n",
        "model.fit(X_train_reshaped, Y_train, epochs=10, batch_size=32, validation_data=(X_test_reshaped, Y_test)) #5)epochs and batch_size manually tuned"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14eoRkpFgHis",
        "outputId": "788cf44b-13f0-4842-87e4-42be82c39e30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "18/18 [==============================] - 19s 384ms/step - loss: 0.6363 - f1_metric: 0.8191 - val_loss: 0.5878 - val_f1_metric: 0.8436\n",
            "Epoch 2/10\n",
            "18/18 [==============================] - 1s 42ms/step - loss: 0.6140 - f1_metric: 0.8274 - val_loss: 0.5891 - val_f1_metric: 0.8436\n",
            "Epoch 3/10\n",
            "18/18 [==============================] - 1s 42ms/step - loss: 0.6108 - f1_metric: 0.8282 - val_loss: 0.5891 - val_f1_metric: 0.8436\n",
            "Epoch 4/10\n",
            "18/18 [==============================] - 1s 41ms/step - loss: 0.6156 - f1_metric: 0.8320 - val_loss: 0.5846 - val_f1_metric: 0.8436\n",
            "Epoch 5/10\n",
            "18/18 [==============================] - 1s 42ms/step - loss: 0.6119 - f1_metric: 0.8255 - val_loss: 0.5877 - val_f1_metric: 0.8436\n",
            "Epoch 6/10\n",
            "18/18 [==============================] - 1s 49ms/step - loss: 0.6066 - f1_metric: 0.8319 - val_loss: 0.5846 - val_f1_metric: 0.8436\n",
            "Epoch 7/10\n",
            "18/18 [==============================] - 1s 41ms/step - loss: 0.6055 - f1_metric: 0.8303 - val_loss: 0.5904 - val_f1_metric: 0.8436\n",
            "Epoch 8/10\n",
            "18/18 [==============================] - 1s 42ms/step - loss: 0.6151 - f1_metric: 0.8275 - val_loss: 0.5872 - val_f1_metric: 0.8436\n",
            "Epoch 9/10\n",
            "18/18 [==============================] - 1s 42ms/step - loss: 0.6114 - f1_metric: 0.8270 - val_loss: 0.5867 - val_f1_metric: 0.8436\n",
            "Epoch 10/10\n",
            "18/18 [==============================] - 1s 48ms/step - loss: 0.6253 - f1_metric: 0.8291 - val_loss: 0.5970 - val_f1_metric: 0.8436\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7effba0a4e50>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There seems to be very little changes for each epoch, let's see if we can get a better result with hyperparameter tuning."
      ],
      "metadata": {
        "id": "hvAqvpLgfByp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperprameter tuning\n"
      ],
      "metadata": {
        "id": "4UgNZsd2ijlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) I will be performing hyperparameter tuning on learning rate, batch size, dropout rate and number of layers using keras tuner"
      ],
      "metadata": {
        "id": "mGey-zMUn8GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "  model = Sequential()\n",
        "  model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "  model.add(Dropout(hp.Float('dropout_1', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "\n",
        "  # Tuning number of layers\n",
        "  for i in range(hp.Int('num_layers', 1, 3)):\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal')))\n",
        "    model.add(Dropout(hp.Float('dropout_' + str(i+1), 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "\n",
        "  model.add(Bidirectional(LSTM(128, kernel_initializer='he_normal')))\n",
        "  model.add(Dropout(hp.Float('dropout_last', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "  model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
        "  model.add(Dropout(hp.Float('dropout_dense', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='binary_crossentropy', metrics=['accuracy']) #Tuning learning rate\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "c2g77juJUG_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal'), input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "    model.add(Dropout(hp.Float('dropout_1', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        model.add(Bidirectional(LSTM(256, return_sequences=True, kernel_initializer='he_normal')))\n",
        "        model.add(Dropout(hp.Float('dropout_' + str(i+1), 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "\n",
        "    model.add(Bidirectional(LSTM(128, kernel_initializer='he_normal')))\n",
        "    model.add(Dropout(hp.Float('dropout_last', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "    model.add(Dense(64, activation='relu', kernel_initializer='he_normal'))\n",
        "    model.add(Dropout(hp.Float('dropout_dense', 0, 0.7, step=0.1))) #Tuning dropout rate\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate), loss='binary_crossentropy', metrics=['accuracy']) #Tuning learning rate\n",
        "\n",
        "    return model\n",
        "\n",
        "shutil.rmtree('my_dir/my_project', ignore_errors=True)\n",
        "\n",
        "tuner = kt.tuners.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=5,\n",
        "    executions_per_trial=3,\n",
        "    directory='my_dir',\n",
        "    project_name='my_project'\n",
        ")\n",
        "\n",
        "\n",
        "def train_model(model, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
        "    history = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_val, y_val))\n",
        "    return history\n",
        "\n",
        "\n",
        "tuner.search(x=X_train_reshaped, y=Y_train,\n",
        "             epochs=10,\n",
        "             batch_size=64,\n",
        "             validation_data=(X_test_reshaped, Y_test),\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(patience=1)])\n",
        "\n",
        "\n",
        "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "Mr5I5GORNKns",
        "outputId": "65711d95-2bfb-4003-8f57-ba1134ed0531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'kt' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-23f177a6f1b4>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_dir/my_project'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m tuner = kt.tuners.RandomSearch(\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mbuild_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'kt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning didn't seem to help the performance"
      ],
      "metadata": {
        "id": "LR8iZQYoQ9u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) To take a look at the overall performance of the model I'm going to evaluate it based on accuracy, precision, recall and f1_score"
      ],
      "metadata": {
        "id": "plV2yrBkufB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "best_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "Y_pred = best_model.predict(X_test_reshaped)\n",
        "Y_pred_classes = (Y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "accuracy = accuracy_score(Y_test, Y_pred_classes)\n",
        "precision = precision_score(Y_test, Y_pred_classes)\n",
        "recall = recall_score(Y_test, Y_pred_classes)\n",
        "f1 = f1_score(Y_test, Y_pred_classes)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\\n Precision:  {precision}\\n Recall: {recall}\\n F1 Score:{f1}\")"
      ],
      "metadata": {
        "id": "wdBEwUX5whNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6) Know we are going to check the performance on stratified k fold cross validation"
      ],
      "metadata": {
        "id": "DedcvVixwhev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 5\n",
        "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kf.split(X_train_reshaped, Y_train):\n",
        "  X_train_fold, X_test_fold = X_train_reshaped[train_idx], X_train_reshaped[test_idx]\n",
        "  Y_train_fold, Y_test_fold = Y_train[train_idx], Y_train[test_idx]\n",
        "\n",
        "  best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "  best_model = tuner.hypermodel.build(best_hp)\n",
        "\n",
        "  best_model.fit(X_train_fold, Y_train_fold, epochs=10, batch_size=32)\n",
        "\n",
        "  Y_pred_fold = best_model.predict(X_test_fold)\n",
        "  Y_pred_classes_fold = (Y_pred_fold > 0.5).astype(\"int32\")\n",
        "\n",
        "  f1_fold = f1_score(Y_test_fold, Y_pred_classes_fold)\n",
        "\n",
        "  f1_scores.append(f1_fold)\n",
        "\n",
        "mean_f1 = np.mean(f1_scores)\n",
        "std_f1 = np.std(f1_scores)\n",
        "\n",
        "\n",
        "print(f\"Mean F1 Score: {mean_f1}\")\n",
        "print(f\"Standard Deviation of F1 Scores: {std_f1}\")"
      ],
      "metadata": {
        "id": "sqbSrh3Be0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation scores seem to be decently high but while running the model it seems that with each epoch the score did not have a lot of variation something that were not mentioned in the exam that could be implemented are: changing model, adding preprocessing steps such as stop word removal, stemming/lemmatizing, name entity recognition to help the classification process"
      ],
      "metadata": {
        "id": "sRFm9gkiVv_e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}